version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434" # Map Ollama's API port to your host
    volumes:
      - ollama_data:/root/.ollama # Persistent storage for models
    restart: unless-stopped
    environment:
      # This allows Ollama to be accessible from other containers in the Docker network
      # and from your host machine.
      - OLLAMA_HOST=0.0.0.0

    # === OPTIONAL: GPU SUPPORT CONFIGURATION ===
    # Uncomment the relevant section below if you have a GPU and want to use it.
    # Ensure you have the necessary drivers and NVIDIA Container Toolkit (for NVIDIA)
    # or are using the 'rocm' image (for AMD) installed on your host.

    # For NVIDIA GPUs (Linux):
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all # or specify a number, e.g., 'count: 1'
    #           capabilities: [gpu]

    # For AMD GPUs (Linux):
    # Change 'image: ollama/ollama:latest' above to 'image: ollama/ollama:rocm'
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: amd
    #           count: all
    #           capabilities: [gpu]

volumes:
  ollama_data:
    driver: local